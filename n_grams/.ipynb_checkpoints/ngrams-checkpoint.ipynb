{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from nltk import word_tokenize, bigrams, trigrams, Counter\n",
    "import pandas as pd\n",
    "from numpy import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input file paths\n",
    "INPUT_FILE = './data/lincoln_speeches_000.txt'\n",
    "\n",
    "# Defining output file paths\n",
    "TOKEN_OUTPUT = './outputs/tokens_percent.csv'\n",
    "BIGRAM_OUTPUT = './outputs/bigrams_pmi.csv'\n",
    "TRIGRAM_OUTPUT = './outputs/trigrams_pmi.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Corpus Details\n",
    "\n",
    "The text file is an extract from the Presidential Speech Corpus:\n",
    "\n",
    "**Speech details:**\n",
    "\n",
    "_Speaker: Abraham Lincoln_\n",
    "\n",
    "_Location: Peoria, Illinois_\n",
    "\n",
    "_Date: October 16, 1854_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Process\n",
    "\n",
    "In order to process the text in the speech file, the file has to be read into the memory. The file text is stored as one string in memory. The following processes are then applied in order to study word distribution, bigrams and bigram probabilities, and trigram and trigram properties:\n",
    "1. Word Tokenization, Token Probability and Logarithmic Token Probability\n",
    "2. Bigram Construction and Association Measures\n",
    "3. Trigram Construction and Association Measures\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in speech, including punctuation: 187,150\n"
     ]
    }
   ],
   "source": [
    "# Reading data from text file\n",
    "with open(INPUT_FILE) as t:\n",
    "    text_data = t.readlines()[2]\n",
    "\n",
    "print(\"Total number of tokens in speech, including punctuation: {0:,}\".format(len(text_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Word Tokenization\n",
    "\n",
    "**Tokenization** or **Word Segmentation** is the task of separating out words from running text.\n",
    "\n",
    "A **token** is a sequence of characters in a particular language or document that can be grouped together as a semantic unit for processing. The process of tokenization allows us to split a document into such individual words or tokens for the purpose of lexical analysis. Tokens have identifiable characteristics (Parts of Speech tags, singular/plural nouns, common/proper nouns, etc.) which can enable easier analysis and identification of context in a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2,746\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text\n",
    "speech_tokens = word_tokenize(text_data)\n",
    "\n",
    "# Creating a toekn distribution dataframe for easier analysis\n",
    "token_dist_df = pd.DataFrame(columns=['token', 'token_count', 'percent_dist', 'log_percent_dist'])\n",
    "for token in speech_tokens:\n",
    "    token_dist_df = token_dist_df.append({'token':token, \n",
    "                                         'token_count':Counter(speech_tokens)[token], \n",
    "                                         'percent_dist':Counter(speech_tokens)[token]/len(text_data), \n",
    "                                         'log_percent_dist': log(Counter(speech_tokens)[token]/len(text_data))}, \n",
    "                                         ignore_index=True)\n",
    "# Dropping duplicates in token distribution dataframe    \n",
    "token_dist_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Saving token distribution dataframe to a csv file\n",
    "token_dist_df.to_csv(TOKEN_OUTPUT)\n",
    "\n",
    "print(\"Number of unique tokens: {0:,}\".format(len(token_dist_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### N-grams\n",
    "\n",
    "An **n-gram** is a sequence of _n_ adjacent tokens in a given document.The tokens may be phonemes, syllables, letters, words or base pairs based on application.\n",
    "\n",
    "N-grams may be used to identify the most common occurences in a given document, or to generate text from a chosen corpus.\n",
    "\n",
    "The _n_ in n-grams corresponds to the number of adjacent tokens that are being analysed or generated. \n",
    "\n",
    "_n=2_ gives us **bigrams**, _n=3_ gives us **trigrams** and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information\n",
    "\n",
    "**Pointwise mutual information** is a measure of association used in statistical analysis and information theory.\n",
    "\n",
    "The Pointwise Mutual Information or PMI for a given pair of outcomes x and y quantifies the discrepency between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence.\n",
    "\n",
    "$$PMI(x,y) = log\\frac{P(x,y)}{P(x)P(y)} = log\\frac{P(x|y)}{P(x)} = log\\frac{P(y|x)}{P(y)}$$\n",
    "\n",
    "PMI is a slightly normalized way of understanding the distribution of n-grams in a given document.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams, Pointwise Mutual Information and Bigram Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "bigram_collocation_finder = BigramCollocationFinder.from_words(speech_tokens)\n",
    "\n",
    "# Storing bigrams sorted by pmi to a dataframe\n",
    "bigrams_df = pd.DataFrame(bigram_collocation_finder.score_ngrams(bigram_measures.pmi), columns=['bigram', 'PMI'])\n",
    "\n",
    "# Saving bigram dataframe to a csv file\n",
    "bigrams_df.to_csv(BIGRAM_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Trigrams, Pointwise Mutual Information and Trigram Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "trigram_collocation_finder = TrigramCollocationFinder.from_words(speech_tokens)\n",
    "\n",
    "# Storing trigrams sorted by pmi to a dataframe\n",
    "trigram_df = pd.DataFrame(trigram_collocation_finder.score_ngrams(trigram_measures.pmi), columns=['trigram', 'score'])\n",
    "\n",
    "# Saving trigram datafrae=me to a csv file\n",
    "trigram_df.to_csv(TRIGRAM_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "\n",
    "### Citations\n",
    "\n",
    "^Brown, D. W. (2016). \"Corpus of Presidential Speeches\". Retrieved from [The Grammar Lab](http://www.thegrammarlab.com).\n",
    "\n",
    "^D. Jurafsky, J. H. Martin (2007). \"Speech and Language Processing\".\n",
    "\n",
    "^Bouma, Gerlof (2009). \"Normalized (Pointwise) Mutual Information in Collocation Extraction\". Proceedings of the Biennial GSCL Conference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
